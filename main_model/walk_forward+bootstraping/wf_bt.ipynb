{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a6d0563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.449509Z",
     "start_time": "2026-01-25T13:59:16.442271Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edef7fa",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12e2f7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.454321Z",
     "start_time": "2026-01-25T13:59:16.452124Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_target(input_df, ticker):\n",
    "    df = input_df.copy()\n",
    "    df[\"Target\"] = (df[f\"Close_{ticker}\"].shift(-1) > df[f\"Close_{ticker}\"]).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5226bedb743735d",
   "metadata": {},
   "source": [
    "# Target TBM ( Triple Barrier Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57cd8af5bb166577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.473252Z",
     "start_time": "2026-01-25T13:59:16.470100Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tbm_target(df, ticker, horizon=5, pt_sl=[1.2,1]):\n",
    "    df = df.copy()\n",
    "    close = df[f'Close_{ticker}']\n",
    "    \n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "    volatility = log_ret.rolling(window=20).std()\n",
    "    \n",
    "    targets = pd.Series(index=df.index, dtype=float)\n",
    "    \n",
    "    for i in range(len(df) - horizon):\n",
    "        price_start = close.iloc[i]\n",
    "        current_vol = volatility.iloc[i] ### dynamic barrier for each day\n",
    "        \n",
    "        upper_barrier = price_start * (1 + current_vol * pt_sl[0])\n",
    "        lower_barrier = price_start * (1 - current_vol * pt_sl[1])\n",
    "        \n",
    "        future_prices = close.iloc[i+1 : i+ 1 + horizon]\n",
    "        \n",
    "        targets.iloc[i] = 0\n",
    "        \n",
    "        for price_future in future_prices:\n",
    "            if price_future >= upper_barrier:\n",
    "                targets.iloc[i] = 1 # profit taking hit\n",
    "                break\n",
    "            elif price_future <= lower_barrier:\n",
    "                targets.iloc[i] = -1 # stop loss hit\n",
    "                break\n",
    "    df['Target'] = targets\n",
    "    return df.dropna(subset=['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f8051",
   "metadata": {},
   "source": [
    "# Model: Ensemble (Voting Soft)\n",
    "### With optuna to optimized"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ef56ab7dab7bd2f9"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d267750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.498065Z",
     "start_time": "2026-01-25T13:59:16.493050Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_ensemble_model_vote(X_train, y_train):\n",
    "    def objective(trial):\n",
    "        # for Random Forest\n",
    "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 50, 200)\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 3, 10)\n",
    "        \n",
    "        # for XGBoost\n",
    "        xgb_learning_rate = trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.2, log=True)\n",
    "        xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 2, 6)\n",
    "        \n",
    "        # for SVM\n",
    "        svm_c = trial.suggest_float(\"svm_c\", 0.1, 10.0, log=True)\n",
    "        \n",
    "        lr_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial' ,random_state=42))\n",
    "    ])\n",
    "        rf_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"rf\", RandomForestClassifier(\n",
    "            n_estimators= rf_n_estimators,\n",
    "            max_depth= rf_max_depth,\n",
    "            min_samples_leaf=10,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])  \n",
    "        svm_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=svm_c,\n",
    "            gamma=\"scale\",\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "        \n",
    "        xgb_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth= xgb_max_depth,\n",
    "            learning_rate= xgb_learning_rate,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\"\n",
    "        ))\n",
    "    ])\n",
    "        \n",
    "        ensemble = VotingClassifier(\n",
    "            estimators= [('lr', lr_pipe),('rf', rf_pipe), ('svm', svm_pipe), ('xgb', xgb_pipe)],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        score = cross_val_score(ensemble, X_train, y_train, cv=tscv, scoring='f1_weighted')\n",
    "        \n",
    "        return score.mean()\n",
    "    \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=30)\n",
    "        \n",
    "    best = study.best_params    \n",
    "    lr_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=1000,\n",
    "                                  solver='lbfgs',\n",
    "                                  #multi_class='multinomial',\n",
    "                                  random_state=42))\n",
    "    ])\n",
    "\n",
    "    rf_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"rf\", RandomForestClassifier(\n",
    "            n_estimators=best['rf_n_estimators'],\n",
    "            max_depth=best['rf_max_depth'],\n",
    "            min_samples_leaf=10,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    svm_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=best['svm_c'],\n",
    "            gamma=\"scale\",\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    xgb_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=best['xgb_max_depth'],\n",
    "            learning_rate=best['xgb_learning_rate'],\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"logloss\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    model = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"lr\", lr_pipeline),\n",
    "            (\"rf\", rf_pipeline),\n",
    "            (\"svm\", svm_pipeline),\n",
    "            (\"xgb\", xgb_pipeline),\n",
    "        ],\n",
    "        voting=\"soft\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f04c9",
   "metadata": {},
   "source": [
    "### Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050efec6740a3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.501813Z",
     "start_time": "2026-01-25T13:59:16.499890Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sample_weights(df, horizon=5):\n",
    "    # create a binary matrix indicating which days are covered by which barrier\n",
    "    num_rows = len(df)\n",
    "    concurrency = np.zeros(num_rows)\n",
    "\n",
    "    for i in range(num_rows - horizon):\n",
    "        concurrency[i  : i + horizon] += 1\n",
    "\n",
    "    uniqueness = 1.0 / np.maximum(concurrency, 1)\n",
    "\n",
    "    weights = pd.Series(index=df.index, dtype=float)\n",
    "    for i in range(num_rows - horizon):\n",
    "        weights.iloc[i] = uniqueness[i : i + horizon].mean()\n",
    "        \n",
    "    return weights.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfdf85e9bc48ba",
   "metadata": {},
   "source": [
    "# Walk Forward validation with purging and embargo in validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821dd77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.515060Z",
     "start_time": "2026-01-25T13:59:16.511052Z"
    }
   },
   "outputs": [],
   "source": [
    "def walk_forward_validation_with_purging(\n",
    "    df,\n",
    "    features,\n",
    "    model_version,\n",
    "    target_col=\"Target\",\n",
    "    date_col=\"DATE\",\n",
    "    start_year=2010,\n",
    "    first_train_end_year=2015,\n",
    "    last_test_year=2023,\n",
    "    horizon = 5,\n",
    "    embargo_pct = 0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Train: start_year -> train_end_year\n",
    "    Test : train_end_year+1\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_proba = []\n",
    "\n",
    "    fold_rows = []\n",
    "\n",
    "    sample_weights_all = get_sample_weights(df, horizon)\n",
    "    \n",
    "    for train_end_year in range(first_train_end_year, last_test_year):\n",
    "        test_year = train_end_year + 1\n",
    "\n",
    "        train_mask = (df[date_col].dt.year >= start_year) & (df[date_col].dt.year <= train_end_year)\n",
    "        test_mask = (df[date_col].dt.year == test_year)\n",
    "\n",
    "        train_df = df[train_mask]\n",
    "        test_df = df[test_mask]\n",
    "\n",
    "        # jeżeli jakiś rok nie ma danych to skip\n",
    "        if len(train_df) < 200 or len(test_df) < 50:\n",
    "            continue\n",
    "\n",
    "        ## Purging\n",
    "        train_df_purged = train_df.iloc[:-horizon]\n",
    "        weights_train = sample_weights_all.loc[train_df_purged.index] \n",
    "        \n",
    "        ## Embargo\n",
    "        embargo_size = int(len(df) * embargo_pct)\n",
    "        test_df_embargo = test_df.iloc[embargo_size:]\n",
    "        \n",
    "        if len(test_df_embargo) < 10: continue\n",
    "        \n",
    "        \n",
    "        X_train = train_df_purged[features]\n",
    "        y_train = train_df_purged[target_col]\n",
    "\n",
    "        X_test = test_df_embargo[features]\n",
    "        y_test = test_df_embargo[target_col]\n",
    "\n",
    "        model = model_version(X_train,  y_train)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test) \n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        auc = roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovr')\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"train_end_year\": train_end_year,\n",
    "            \"test_year\": test_year,\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_test\": len(test_df),\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"roc_auc\": auc\n",
    "        })\n",
    "\n",
    "        all_y_true.extend(y_test.tolist())\n",
    "        all_y_pred.extend(y_pred.tolist())\n",
    "        all_y_proba.extend(y_proba.tolist())\n",
    "\n",
    "    folds_df = pd.DataFrame(fold_rows)\n",
    "\n",
    "    return folds_df, np.array(all_y_true), np.array(all_y_pred), np.array(all_y_proba)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70340a",
   "metadata": {},
   "source": [
    "# Block bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b22d0d17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.527465Z",
     "start_time": "2026-01-25T13:59:16.524726Z"
    }
   },
   "outputs": [],
   "source": [
    "def block_bootstrap_accuracy(y_true, y_pred, block_size=20, n_bootstrap=1000, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(y_true)\n",
    "\n",
    "    if n < block_size:\n",
    "        raise ValueError(\"Za mało danych do bootstrapa w tej konfiguracji.\")\n",
    "\n",
    "    acc_samples = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        sampled_idx = []\n",
    "\n",
    "        while len(sampled_idx) < n:\n",
    "            start = rng.integers(0, n - block_size + 1)\n",
    "            block = list(range(start, start + block_size))\n",
    "            sampled_idx.extend(block)\n",
    "\n",
    "        sampled_idx = sampled_idx[:n]\n",
    "        y_true_bs = y_true[sampled_idx]\n",
    "        y_pred_bs = y_pred[sampled_idx]\n",
    "\n",
    "        acc = accuracy_score(y_true_bs, y_pred_bs)\n",
    "        acc_samples.append(acc)\n",
    "\n",
    "    acc_samples = np.array(acc_samples)\n",
    "    ci_low = np.percentile(acc_samples, 2.5)\n",
    "    ci_high = np.percentile(acc_samples, 97.5)\n",
    "\n",
    "    return acc_samples, ci_low, ci_high\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe72a57",
   "metadata": {},
   "source": [
    "# Walk_forward + Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3626543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T13:59:16.541337Z",
     "start_time": "2026-01-25T13:59:16.538069Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_stage4_for_ticker(df_raw, ticker, selected_features, model_version):\n",
    "    df = get_tbm_target(df_raw, ticker)\n",
    "\n",
    "    selected_features = [f for f in selected_features if f in df.columns]\n",
    "\n",
    "    folds_df, y_true_all, y_pred_all, y_proba_all = walk_forward_validation_with_purging(\n",
    "        df=df,\n",
    "        features=selected_features,\n",
    "        model_version=model_version,\n",
    "        target_col=\"Target\",\n",
    "        date_col=\"DATE\",\n",
    "        start_year=2010,\n",
    "        first_train_end_year=2015,\n",
    "        last_test_year=2023 \n",
    "    )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" WALK-FORWARD RESULTS for {ticker}\")\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    summary_mean = folds_df[metrics].mean()\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(summary_mean.to_frame(name=\"Mean\").T)\n",
    "     \n",
    "    print(\"\\n--- Detailed Classification Report (Whole Period) ---\")    \n",
    "    print(classification_report(y_true_all, y_pred_all, zero_division=0))\n",
    "    \n",
    "    # Block Bootstrap\n",
    "    acc_samples, ci_low, ci_high = block_bootstrap_accuracy(\n",
    "        y_true=y_true_all,\n",
    "        y_pred=y_pred_all,\n",
    "        block_size=20,\n",
    "        n_bootstrap=1000\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\" BLOCK BOOTSTRAP for {ticker}\")\n",
    "    print(f\"95% CI accuracy: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "    print(f\"Bootstrap mean accuracy: {acc_samples.mean():.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Loss (-1)', 'Neutral (0)', 'Profit (1)'],\n",
    "                yticklabels=['Loss (-1)', 'Neutral (0)', 'Profit (1)'])\n",
    "    plt.title(f'Confusion Matrix for {ticker}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return folds_df, acc_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fdf69",
   "metadata": {},
   "source": [
    "# Example use"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "tickers = [\"AAPL\", \"GOOGL\", \"MSFT\"]\n",
    "\n",
    "with open(\"../../selected_features/feature_dict.json\", \"r\") as f:\n",
    "    feature_dict = json.load(f)\n",
    "\n",
    "\n",
    "def get_all_features(df):\n",
    "    return [c for c in df.columns if c not in [\"DATE\", \"index\", \"Target\"]]\n",
    "\n",
    "for share in tickers:\n",
    "    data = pd.read_csv(f\"../../data/all_data/all_{share}_data.csv\")\n",
    "    df_tmp = get_tbm_target(data, share)\n",
    "    features_aapl = get_all_features(df_tmp)\n",
    "    print(f\"------{share}-----\")\n",
    "        \n",
    "    selected_features = feature_dict[share]\n",
    "    print(f\"Selected features for {share}:\")\n",
    "    # print(f\"Running for {share} with all features ({len(features_aapl)})\")\n",
    "    # run_stage4_for_ticker(data, share, features_aapl)\n",
    "    print(f\"Running for {share} with selected features ({len(selected_features)})\")\n",
    "    run_stage4_for_ticker(data, share, selected_features, model_version=build_ensemble_model_vote)"
   ],
   "id": "c6d2e952964e3a3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
